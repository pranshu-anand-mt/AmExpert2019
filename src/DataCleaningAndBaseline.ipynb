{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip --quiet\n",
    "!pip install -r requiremen-upgrade pip --quiet\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score as roc_auc_score\n",
    "\n",
    "\n",
    "!pip install matplotlib --quiet\n",
    "!pip freeze --quiet > ../requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "#     current_file = os.path.abspath(os.path.dirname(__file__))\n",
    "    current_file = \"\"\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/campaign_data.csv')\n",
    "    campaign_data = pd.read_csv('../training_data/campaign_data.csv')\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/coupon_item_mapping.csv')\n",
    "    coupon_item_mapping_data = pd.read_csv(csv_filename)\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/customer_demographics.csv')\n",
    "    customer_demographics_data = pd.read_csv(csv_filename)\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/customer_transaction_data.csv')\n",
    "    customer_transaction_data = pd.read_csv(csv_filename)\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/item_data.csv')\n",
    "    item_data = pd.read_csv(csv_filename)\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../training_data/train.csv')\n",
    "    train_data = pd.read_csv(csv_filename)\n",
    "\n",
    "    csv_filename = os.path.join(current_file, '../test_data/test_QyjYwdj.csv')\n",
    "    test_data = pd.read_csv('../test_data/test_QyjYwdj.csv')\n",
    "    \n",
    "    return campaign_data, coupon_item_mapping_data, customer_demographics_data, \\\n",
    "           customer_transaction_data, item_data, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_data, coupon_item_mapping_data, customer_demographics_data, \\\n",
    "customer_transaction_data, item_data, train_data, test_data = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_analysis(df):\n",
    "    print(\"\\n Head: \")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\n Number of NaN Entries: \")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    print(\"\\n Number of Null Entries: \")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\n Statistical Description: \")\n",
    "    print(df.describe(include='all'))\n",
    "   \n",
    "    for column in df.columns.values:\n",
    "        print(\"\\n\\n Uniqueness analysis for column: \", column)\n",
    "        unique_values = df[column].unique()\n",
    "        print(\"\\n\\t\\t Number of Unique values: \", len(unique_values))\n",
    "        print(\"\\n\\t\\t Unique values: \", unique_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------- campaign_data --------------------------------------- \n",
      "\n",
      " campaign_data_columns:  ['campaign_id' 'campaign_type' 'start_date' 'end_date']\n",
      "\n",
      " Head: \n",
      "   campaign_id campaign_type start_date  end_date\n",
      "0           24             Y   21/10/13  20/12/13\n",
      "1           25             Y   21/10/13  22/11/13\n",
      "2           20             Y   07/09/13  16/11/13\n",
      "3           23             Y   08/10/13  15/11/13\n",
      "4           21             Y   16/09/13  18/10/13\n",
      "\n",
      " Number of NaN Entries: \n",
      "campaign_id      0\n",
      "campaign_type    0\n",
      "start_date       0\n",
      "end_date         0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "campaign_id      0\n",
      "campaign_type    0\n",
      "start_date       0\n",
      "end_date         0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "        campaign_id campaign_type start_date  end_date\n",
      "count     28.000000            28         28        28\n",
      "unique          NaN             2         25        26\n",
      "top             NaN             Y   21/10/13  18/01/13\n",
      "freq            NaN            22          2         2\n",
      "mean      15.571429           NaN        NaN       NaN\n",
      "std        9.118271           NaN        NaN       NaN\n",
      "min        1.000000           NaN        NaN       NaN\n",
      "25%        7.750000           NaN        NaN       NaN\n",
      "50%       16.500000           NaN        NaN       NaN\n",
      "75%       23.250000           NaN        NaN       NaN\n",
      "max       30.000000           NaN        NaN       NaN\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  campaign_id\n",
      "\n",
      "\t\t Number of Unique values:  28\n",
      "\n",
      "\t\t Unique values:  [24 25 20 23 21 22 18 19 17 16 13 11 12 10  9  8  7  6  3  5  4  1  2 30\n",
      " 29 28 27 26]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  campaign_type\n",
      "\n",
      "\t\t Number of Unique values:  2\n",
      "\n",
      "\t\t Unique values:  ['Y' 'X']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  start_date\n",
      "\n",
      "\t\t Number of Unique values:  25\n",
      "\n",
      "\t\t Unique values:  ['21/10/13' '07/09/13' '08/10/13' '16/09/13' '10/08/13' '26/08/13'\n",
      " '29/07/13' '15/07/13' '19/05/13' '22/04/13' '08/04/13' '11/03/13'\n",
      " '16/02/13' '02/02/13' '28/01/13' '22/12/12' '12/01/13' '07/01/13'\n",
      " '12/12/12' '17/12/12' '19/11/12' '08/10/12' '16/09/12' '25/08/12'\n",
      " '12/08/12']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  end_date\n",
      "\n",
      "\t\t Number of Unique values:  26\n",
      "\n",
      "\t\t Unique values:  ['20/12/13' '22/11/13' '16/11/13' '15/11/13' '18/10/13' '04/10/13'\n",
      " '27/09/13' '30/08/13' '16/08/13' '05/07/13' '07/06/13' '24/05/13'\n",
      " '10/05/13' '12/04/13' '05/04/13' '08/03/13' '01/03/13' '16/02/13'\n",
      " '15/02/13' '08/02/13' '18/01/13' '04/01/13' '30/11/12' '16/11/12'\n",
      " '27/10/12' '21/09/12']\n",
      "   campaign_id campaign_type start_date   end_date\n",
      "0           24             Y 2013-10-21 2013-12-20\n",
      "1           25             Y 2013-10-21 2013-11-22\n",
      "2           20             Y 2013-09-07 2013-11-16\n",
      "3           23             Y 2013-10-08 2013-11-15\n",
      "4           21             Y 2013-09-16 2013-10-18\n"
     ]
    }
   ],
   "source": [
    "# campaign_data data cleaning and imputation\n",
    "\n",
    "\n",
    "print(\"\\n -------------------------------------- campaign_data --------------------------------------- \")\n",
    "campaign_data_columns =  campaign_data.columns.values\n",
    "print(\"\\n campaign_data_columns: \", campaign_data_columns)\n",
    "\n",
    "univariate_analysis(campaign_data)\n",
    "\n",
    "campaign_data['start_date'] = pd.to_datetime(campaign_data.start_date, format=\"%d/%m/%y\")\n",
    "campaign_data['end_date'] = pd.to_datetime(campaign_data.end_date, format=\"%d/%m/%y\")\n",
    "\n",
    "print(campaign_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------- coupon_item_mapping_data --------------------------------------- \n",
      "\n",
      " coupon_item_mapping_data_columns:  ['coupon_id' 'item_id']\n",
      "\n",
      " Head: \n",
      "   coupon_id  item_id\n",
      "0        105       37\n",
      "1        107       75\n",
      "2        494       76\n",
      "3        522       77\n",
      "4        518       77\n",
      "\n",
      " Number of NaN Entries: \n",
      "coupon_id    0\n",
      "item_id      0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "coupon_id    0\n",
      "item_id      0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "          coupon_id       item_id\n",
      "count  92663.000000  92663.000000\n",
      "mean     155.967387  36508.613071\n",
      "std      282.991720  21131.312716\n",
      "min        1.000000      1.000000\n",
      "25%       22.000000  18255.500000\n",
      "50%       30.000000  37955.000000\n",
      "75%       42.000000  54191.500000\n",
      "max     1116.000000  74061.000000\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  coupon_id\n",
      "\n",
      "\t\t Number of Unique values:  1116\n",
      "\n",
      "\t\t Unique values:  [105 107 494 ... 217 218 219]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  item_id\n",
      "\n",
      "\t\t Number of Unique values:  36289\n",
      "\n",
      "\t\t Unique values:  [   37    75    76 ... 71965 67815 68920]\n",
      "   coupon_id  item_id\n",
      "0        105       37\n",
      "1        107       75\n",
      "2        494       76\n",
      "3        522       77\n",
      "4        518       77\n"
     ]
    }
   ],
   "source": [
    "# coupon_item_mapping_data data cleaning and imputation\n",
    "\n",
    "print(\"\\n -------------------------------------- coupon_item_mapping_data --------------------------------------- \")\n",
    "coupon_item_mapping_data_columns =  coupon_item_mapping_data.columns.values\n",
    "print(\"\\n coupon_item_mapping_data_columns: \", coupon_item_mapping_data_columns)\n",
    "\n",
    "univariate_analysis(coupon_item_mapping_data)\n",
    "\n",
    "print(coupon_item_mapping_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------- customer_transaction_data --------------------------------------- \n",
      "\n",
      " customer_transaction_data_columns:  ['date' 'customer_id' 'item_id' 'quantity' 'selling_price'\n",
      " 'other_discount' 'coupon_discount']\n",
      "\n",
      " Head: \n",
      "         date  customer_id  item_id  quantity  selling_price  other_discount  \\\n",
      "0  2012-01-02         1501    26830         1          35.26          -10.69   \n",
      "1  2012-01-02         1501    54253         1          53.43          -13.89   \n",
      "2  2012-01-02         1501    31962         1         106.50          -14.25   \n",
      "3  2012-01-02         1501    33647         1          67.32            0.00   \n",
      "4  2012-01-02         1501    48199         1          71.24          -28.14   \n",
      "\n",
      "   coupon_discount  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "\n",
      " Number of NaN Entries: \n",
      "date               0\n",
      "customer_id        0\n",
      "item_id            0\n",
      "quantity           0\n",
      "selling_price      0\n",
      "other_discount     0\n",
      "coupon_discount    0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "date               0\n",
      "customer_id        0\n",
      "item_id            0\n",
      "quantity           0\n",
      "selling_price      0\n",
      "other_discount     0\n",
      "coupon_discount    0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "              date   customer_id       item_id      quantity  selling_price  \\\n",
      "count      1324566  1.324566e+06  1.324566e+06  1.324566e+06   1.324566e+06   \n",
      "unique         549           NaN           NaN           NaN            NaN   \n",
      "top     2012-09-03           NaN           NaN           NaN            NaN   \n",
      "freq          4753           NaN           NaN           NaN            NaN   \n",
      "mean           NaN  8.040020e+02  2.951903e+04  1.306633e+02   1.146036e+02   \n",
      "std            NaN  4.573363e+02  1.790806e+04  1.311545e+03   1.529053e+02   \n",
      "min            NaN  1.000000e+00  1.000000e+00  1.000000e+00   3.600000e-01   \n",
      "25%            NaN  4.180000e+02  1.468400e+04  1.000000e+00   4.916000e+01   \n",
      "50%            NaN  8.010000e+02  2.659700e+04  1.000000e+00   7.801000e+01   \n",
      "75%            NaN  1.198000e+03  4.240575e+04  1.000000e+00   1.243100e+02   \n",
      "max            NaN  1.582000e+03  7.406600e+04  8.963800e+04   1.780964e+04   \n",
      "\n",
      "        other_discount  coupon_discount  \n",
      "count     1.324566e+06     1.324566e+06  \n",
      "unique             NaN              NaN  \n",
      "top                NaN              NaN  \n",
      "freq               NaN              NaN  \n",
      "mean     -1.776871e+01    -5.948983e-01  \n",
      "std       3.788867e+01     7.069367e+00  \n",
      "min      -3.120310e+03    -1.992230e+03  \n",
      "25%      -2.315000e+01     0.000000e+00  \n",
      "50%      -1.780000e+00     0.000000e+00  \n",
      "75%       0.000000e+00     0.000000e+00  \n",
      "max       0.000000e+00     0.000000e+00  \n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  date\n",
      "\n",
      "\t\t Number of Unique values:  549\n",
      "\n",
      "\t\t Unique values:  ['2012-01-02' '2012-01-03' '2012-01-04' '2012-01-05' '2012-01-06'\n",
      " '2012-01-07' '2012-01-08' '2012-01-09' '2012-01-10' '2012-01-11'\n",
      " '2012-01-12' '2012-01-13' '2012-01-14' '2012-01-15' '2012-01-16'\n",
      " '2012-01-17' '2012-01-18' '2012-01-19' '2012-01-20' '2012-01-21'\n",
      " '2012-01-22' '2012-01-23' '2012-01-24' '2012-01-25' '2012-01-26'\n",
      " '2012-01-27' '2012-01-28' '2012-01-29' '2012-01-30' '2012-01-31'\n",
      " '2012-02-01' '2012-02-02' '2012-02-03' '2012-02-04' '2012-02-05'\n",
      " '2012-02-06' '2012-02-07' '2012-02-08' '2012-02-09' '2012-02-10'\n",
      " '2012-02-11' '2012-02-12' '2012-02-13' '2012-02-14' '2012-02-15'\n",
      " '2012-02-16' '2012-02-17' '2012-02-18' '2012-02-19' '2012-02-20'\n",
      " '2012-02-21' '2012-02-22' '2012-02-23' '2012-02-24' '2012-02-25'\n",
      " '2012-02-26' '2012-02-27' '2012-02-28' '2012-02-29' '2012-03-01'\n",
      " '2012-03-02' '2012-03-03' '2012-03-04' '2012-03-05' '2012-03-06'\n",
      " '2012-03-07' '2012-03-08' '2012-03-09' '2012-03-10' '2012-03-11'\n",
      " '2012-03-12' '2012-03-13' '2012-03-14' '2012-03-15' '2012-03-16'\n",
      " '2012-03-17' '2012-03-18' '2012-03-19' '2012-03-20' '2012-03-21'\n",
      " '2012-03-22' '2012-03-23' '2012-03-24' '2012-03-25' '2012-03-26'\n",
      " '2012-03-27' '2012-03-28' '2012-03-29' '2012-03-30' '2012-03-31'\n",
      " '2012-04-01' '2012-04-02' '2012-04-03' '2012-04-04' '2012-04-05'\n",
      " '2012-04-06' '2012-04-07' '2012-04-08' '2012-04-09' '2012-04-10'\n",
      " '2012-04-11' '2012-04-12' '2012-04-13' '2012-04-14' '2012-04-15'\n",
      " '2012-04-16' '2012-04-17' '2012-04-18' '2012-04-19' '2012-04-20'\n",
      " '2012-04-21' '2012-04-22' '2012-04-23' '2012-04-24' '2012-04-25'\n",
      " '2012-04-26' '2012-04-27' '2012-04-28' '2012-04-29' '2012-04-30'\n",
      " '2012-05-01' '2012-05-02' '2012-05-03' '2012-05-04' '2012-05-05'\n",
      " '2012-05-06' '2012-05-07' '2012-05-08' '2012-05-09' '2012-05-10'\n",
      " '2012-05-11' '2012-05-12' '2012-05-13' '2012-05-14' '2012-05-15'\n",
      " '2012-05-16' '2012-05-17' '2012-05-18' '2012-05-19' '2012-05-20'\n",
      " '2012-05-21' '2012-05-22' '2012-05-23' '2012-05-24' '2012-05-25'\n",
      " '2012-05-26' '2012-05-27' '2012-05-28' '2012-05-29' '2012-05-30'\n",
      " '2012-05-31' '2012-06-01' '2012-06-02' '2012-06-03' '2012-06-04'\n",
      " '2012-06-05' '2012-06-06' '2012-06-07' '2012-06-08' '2012-06-09'\n",
      " '2012-06-10' '2012-06-11' '2012-06-12' '2012-06-13' '2012-06-14'\n",
      " '2012-06-15' '2012-06-16' '2012-06-17' '2012-06-18' '2012-06-19'\n",
      " '2012-06-20' '2012-06-21' '2012-06-22' '2012-06-23' '2012-06-24'\n",
      " '2012-06-25' '2012-06-26' '2012-06-27' '2012-06-28' '2012-06-29'\n",
      " '2012-06-30' '2012-07-01' '2012-07-02' '2012-07-03' '2012-07-04'\n",
      " '2012-07-05' '2012-07-06' '2012-07-07' '2012-07-08' '2012-07-09'\n",
      " '2012-07-10' '2012-07-11' '2012-07-12' '2012-07-13' '2012-07-14'\n",
      " '2012-07-15' '2012-07-16' '2012-07-17' '2012-07-18' '2012-07-19'\n",
      " '2012-07-20' '2012-07-22' '2012-07-21' '2012-07-23' '2012-07-24'\n",
      " '2012-07-25' '2012-07-26' '2012-07-27' '2012-07-28' '2012-07-29'\n",
      " '2012-07-30' '2012-07-31' '2012-08-01' '2012-08-02' '2012-08-03'\n",
      " '2012-08-04' '2012-08-05' '2012-08-06' '2012-08-07' '2012-08-08'\n",
      " '2012-08-09' '2012-08-10' '2012-08-11' '2012-08-12' '2012-08-13'\n",
      " '2012-08-14' '2012-08-15' '2012-08-16' '2012-08-27' '2012-08-26'\n",
      " '2012-08-28' '2012-08-17' '2012-08-18' '2012-08-20' '2012-08-19'\n",
      " '2012-08-21' '2012-08-22' '2012-08-23' '2012-08-24' '2012-08-25'\n",
      " '2012-08-29' '2012-08-30' '2012-08-31' '2012-09-01' '2012-09-02'\n",
      " '2012-09-03' '2012-09-04' '2012-09-05' '2012-09-06' '2012-09-07'\n",
      " '2012-09-08' '2012-09-09' '2012-09-10' '2012-09-11' '2012-09-12'\n",
      " '2012-09-13' '2012-09-14' '2012-09-15' '2012-09-16' '2012-09-17'\n",
      " '2012-09-18' '2012-09-19' '2012-09-20' '2012-09-21' '2012-09-22'\n",
      " '2012-09-23' '2012-09-24' '2012-09-25' '2012-09-26' '2012-09-27'\n",
      " '2012-09-28' '2012-09-29' '2012-09-30' '2012-10-01' '2012-10-02'\n",
      " '2012-10-03' '2012-10-04' '2012-10-05' '2012-10-06' '2012-10-07'\n",
      " '2012-10-08' '2012-10-09' '2012-10-10' '2012-10-11' '2012-10-12'\n",
      " '2012-10-13' '2012-10-14' '2012-10-15' '2012-10-16' '2012-10-17'\n",
      " '2012-10-18' '2012-10-19' '2012-10-20' '2012-10-21' '2012-10-22'\n",
      " '2012-10-23' '2012-10-24' '2012-10-25' '2012-10-26' '2012-10-27'\n",
      " '2012-10-28' '2012-10-29' '2012-10-30' '2012-11-01' '2012-10-31'\n",
      " '2012-11-02' '2012-11-03' '2012-11-04' '2012-11-05' '2012-11-06'\n",
      " '2012-11-07' '2012-11-08' '2012-11-09' '2012-11-10' '2012-11-11'\n",
      " '2012-11-12' '2012-11-13' '2012-11-14' '2012-11-15' '2012-11-16'\n",
      " '2012-11-17' '2012-11-18' '2012-11-19' '2012-11-20' '2012-11-21'\n",
      " '2012-11-22' '2012-11-23' '2012-11-24' '2012-11-25' '2012-11-26'\n",
      " '2012-11-27' '2012-11-28' '2012-11-29' '2012-11-30' '2012-12-01'\n",
      " '2012-12-02' '2012-12-03' '2012-12-04' '2012-12-05' '2012-12-06'\n",
      " '2012-12-07' '2012-12-08' '2012-12-09' '2012-12-10' '2012-12-11'\n",
      " '2012-12-12' '2012-12-13' '2012-12-14' '2012-12-15' '2012-12-16'\n",
      " '2012-12-17' '2012-12-18' '2012-12-19' '2012-12-20' '2012-12-21'\n",
      " '2012-12-22' '2012-12-23' '2012-12-24' '2012-12-25' '2012-12-26'\n",
      " '2012-12-27' '2012-12-28' '2012-12-29' '2012-12-30' '2012-12-31'\n",
      " '2013-01-01' '2013-01-02' '2013-01-03' '2013-01-04' '2013-01-05'\n",
      " '2013-01-06' '2013-01-07' '2013-01-08' '2013-01-09' '2013-01-10'\n",
      " '2013-01-11' '2013-01-12' '2013-01-13' '2013-01-14' '2013-01-15'\n",
      " '2013-01-16' '2013-01-17' '2013-01-18' '2013-01-19' '2013-01-20'\n",
      " '2013-01-21' '2013-01-22' '2013-01-23' '2013-01-24' '2013-01-25'\n",
      " '2013-01-26' '2013-01-27' '2013-01-28' '2013-01-29' '2013-01-30'\n",
      " '2013-01-31' '2013-02-01' '2013-02-02' '2013-02-03' '2013-02-04'\n",
      " '2013-02-05' '2013-02-06' '2013-02-07' '2013-02-08' '2013-02-09'\n",
      " '2013-02-10' '2013-02-11' '2013-02-12' '2013-02-13' '2013-02-14'\n",
      " '2013-02-15' '2013-02-16' '2013-02-17' '2013-02-18' '2013-02-19'\n",
      " '2013-02-20' '2013-02-21' '2013-02-22' '2013-02-23' '2013-02-24'\n",
      " '2013-02-25' '2013-02-26' '2013-02-27' '2013-02-28' '2013-03-01'\n",
      " '2013-03-02' '2013-03-03' '2013-03-04' '2013-03-05' '2013-03-06'\n",
      " '2013-03-07' '2013-03-08' '2013-03-09' '2013-03-10' '2013-03-11'\n",
      " '2013-03-12' '2013-03-13' '2013-03-14' '2013-03-15' '2013-03-16'\n",
      " '2013-03-17' '2013-03-18' '2013-03-19' '2013-03-20' '2013-03-21'\n",
      " '2013-03-22' '2013-03-25' '2013-03-29' '2013-03-23' '2013-03-24'\n",
      " '2013-03-26' '2013-03-27' '2013-03-28' '2013-03-30' '2013-03-31'\n",
      " '2013-04-02' '2013-04-03' '2013-04-01' '2013-04-04' '2013-04-05'\n",
      " '2013-04-06' '2013-04-07' '2013-04-08' '2013-04-09' '2013-04-10'\n",
      " '2013-04-11' '2013-04-12' '2013-04-13' '2013-04-14' '2013-04-15'\n",
      " '2013-04-16' '2013-04-17' '2013-04-18' '2013-04-19' '2013-04-20'\n",
      " '2013-04-21' '2013-04-22' '2013-04-23' '2013-04-24' '2013-04-25'\n",
      " '2013-04-26' '2013-04-27' '2013-04-28' '2013-04-29' '2013-04-30'\n",
      " '2013-05-01' '2013-05-02' '2013-05-03' '2013-05-04' '2013-05-05'\n",
      " '2013-05-06' '2013-05-07' '2013-05-08' '2013-05-09' '2013-05-10'\n",
      " '2013-05-11' '2013-05-12' '2013-05-13' '2013-05-14' '2013-05-15'\n",
      " '2013-05-16' '2013-05-17' '2013-05-18' '2013-05-19' '2013-05-20'\n",
      " '2013-05-21' '2013-05-22' '2013-05-23' '2013-05-24' '2013-05-25'\n",
      " '2013-05-26' '2013-05-27' '2013-05-28' '2013-05-29' '2013-05-30'\n",
      " '2013-05-31' '2013-06-01' '2013-06-02' '2013-06-03' '2013-06-04'\n",
      " '2013-06-05' '2013-06-06' '2013-06-07' '2013-06-08' '2013-06-09'\n",
      " '2013-06-10' '2013-06-11' '2013-06-12' '2013-06-13' '2013-06-14'\n",
      " '2013-06-15' '2013-06-16' '2013-06-17' '2013-06-18' '2013-06-19'\n",
      " '2013-06-20' '2013-06-21' '2013-06-22' '2013-06-23' '2013-06-24'\n",
      " '2013-06-25' '2013-06-26' '2013-06-27' '2013-06-28' '2013-06-29'\n",
      " '2013-06-30' '2013-07-01' '2013-07-02' '2013-07-03']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  customer_id\n",
      "\n",
      "\t\t Number of Unique values:  1582\n",
      "\n",
      "\t\t Unique values:  [1501  857   67 ...  405 1569  991]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  item_id\n",
      "\n",
      "\t\t Number of Unique values:  74063\n",
      "\n",
      "\t\t Unique values:  [26830 54253 31962 ...  2777  2953  2971]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  quantity\n",
      "\n",
      "\t\t Number of Unique values:  9252\n",
      "\n",
      "\t\t Unique values:  [    1     3     2 ... 15853 21837 19364]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  selling_price\n",
      "\n",
      "\t\t Number of Unique values:  4923\n",
      "\n",
      "\t\t Unique values:  [  35.26   53.43  106.5  ... 1170.12 1338.6  1623.92]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  other_discount\n",
      "\n",
      "\t\t Number of Unique values:  1418\n",
      "\n",
      "\t\t Unique values:  [  -10.69   -13.89   -14.25 ...  -720.95 -1141.98  -323.07]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  coupon_discount\n",
      "\n",
      "\t\t Number of Unique values:  232\n",
      "\n",
      "\t\t Unique values:  [    0.     -35.62   -14.25   -26.71   -21.02   -19.59   -80.14   -17.81\n",
      "    -8.9   -102.94   -71.24   -21.37   -10.69   -12.47  -178.1   -106.86\n",
      "   -53.43   -35.26   -12.11  -213.72   -56.64   -28.14   -44.52  -569.92\n",
      "   -53.07   -62.33   -49.87   -37.4    -24.93   -32.06  -213.36  -355.84\n",
      "  -142.12   -17.45   -39.18   -40.96  -142.48  -109.71    -7.12   -88.69\n",
      "  -427.44   -45.95   -89.05   -16.03  -113.63  -106.5    -70.53   -31.35\n",
      "   -58.77  -117.55   -60.2    -78.36  -128.23   -23.51   -79.79   -28.5\n",
      "   -92.26  -177.74   -47.73  -554.96   -27.07    -5.34  -498.32   -90.83\n",
      "  -108.64   -95.11  -319.87   -30.28  -284.6    -70.88  -124.67  -152.81\n",
      "   -60.55  -356.2    -91.9   -512.57  -113.27  -184.16  -117.19  -160.29\n",
      "  -184.87  -249.34   -55.21   -95.82   -42.74  -195.55   -97.96   -94.39\n",
      "   -23.15   -27.78  -110.42   -42.39  -127.88   -30.63  -277.48   -69.1\n",
      "   -78.01  -199.12  -248.98   -23.87   -43.46   -89.41   -64.12  -211.23\n",
      "   -38.47   -41.32  -169.91  -111.85   -48.09   -82.64  -284.96  -177.39\n",
      "  -134.64   -47.37   -46.31 -1068.24   -63.76  -320.22  -103.3    -49.51\n",
      "  -120.75   -67.32   -59.49  -334.12   -38.83  -234.38  -509.01   -42.03\n",
      "  -641.16  -712.4   -102.59  -266.79  -135.    -124.31  -426.73  -227.26\n",
      "   -47.02  -115.76   -83.35  -534.3    -81.57  -519.34  -101.16   -24.58\n",
      "  -118.97   -84.78   -99.74   -99.38  -712.04  -154.59  -120.4    -82.99\n",
      "  -237.23   -74.45   -80.5   -163.14  -131.44  -176.32   -84.42  -142.84\n",
      "  -159.93  -133.57   -85.13  -284.25  -167.06   -99.02   -85.49 -1992.23\n",
      "  -213.01   -72.31   -52.36  -191.64   -26.36   -89.76  -427.08  -106.15\n",
      "   -65.54  -533.59  -604.83  -163.5   -245.07  -202.68   -33.84  -509.37\n",
      "  -208.38  -354.42  -283.54  -141.77   -95.46   -39.89  -170.62  -462.7\n",
      "   -65.9   -137.14   -11.75   -66.97  -126.09   -63.05   -56.28  -149.25\n",
      "  -391.46  -259.67  -215.5   -235.09  -122.89   -43.1   -195.91    -3.56\n",
      "  -238.3   -191.99   -62.69 -1211.08  -267.15   -44.88  -298.85   -70.17\n",
      "  -320.58  -219.42   -13.89   -24.22  -366.53  -236.87  -747.66  -125.38\n",
      "  -102.23  -179.52  -136.78  -206.6   -105.79  -112.92   -96.17   -31.7 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  customer_id  item_id  quantity  selling_price  other_discount  \\\n",
      "0 2012-01-02         1501    26830         1          35.26          -10.69   \n",
      "1 2012-01-02         1501    54253         1          53.43          -13.89   \n",
      "2 2012-01-02         1501    31962         1         106.50          -14.25   \n",
      "3 2012-01-02         1501    33647         1          67.32            0.00   \n",
      "4 2012-01-02         1501    48199         1          71.24          -28.14   \n",
      "\n",
      "   coupon_discount  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n"
     ]
    }
   ],
   "source": [
    "# customer_transaction_data data cleaning and imputation\n",
    "\n",
    "print(\"\\n -------------------------------------- customer_transaction_data --------------------------------------- \")\n",
    "customer_transaction_data_columns =  customer_transaction_data.columns.values\n",
    "print(\"\\n customer_transaction_data_columns: \", customer_transaction_data_columns)\n",
    "univariate_analysis(customer_transaction_data)\n",
    "\n",
    "customer_transaction_data['date'] = pd.to_datetime(customer_transaction_data.date, format=\"%Y/%m/%d\")\n",
    "\n",
    "print(customer_transaction_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------- item_data --------------------------------------- \n",
      "\n",
      " item_data_columns:  ['item_id' 'brand' 'brand_type' 'category']\n",
      "\n",
      " Head: \n",
      "   item_id  brand   brand_type       category\n",
      "0        1      1  Established        Grocery\n",
      "1        2      1  Established  Miscellaneous\n",
      "2        3     56        Local         Bakery\n",
      "3        4     56        Local        Grocery\n",
      "4        5     56        Local        Grocery\n",
      "\n",
      " Number of NaN Entries: \n",
      "item_id       0\n",
      "brand         0\n",
      "brand_type    0\n",
      "category      0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "item_id       0\n",
      "brand         0\n",
      "brand_type    0\n",
      "category      0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "             item_id         brand   brand_type category\n",
      "count   74066.000000  74066.000000        74066    74066\n",
      "unique           NaN           NaN            2       19\n",
      "top              NaN           NaN  Established  Grocery\n",
      "freq             NaN           NaN        62842    32448\n",
      "mean    37033.500000   1485.560055          NaN      NaN\n",
      "std     21381.156856   1537.385673          NaN      NaN\n",
      "min         1.000000      1.000000          NaN      NaN\n",
      "25%     18517.250000    278.000000          NaN      NaN\n",
      "50%     37033.500000    978.000000          NaN      NaN\n",
      "75%     55549.750000   2013.000000          NaN      NaN\n",
      "max     74066.000000   5528.000000          NaN      NaN\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  item_id\n",
      "\n",
      "\t\t Number of Unique values:  74066\n",
      "\n",
      "\t\t Unique values:  [    1     2     3 ... 74064 74065 74066]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  brand\n",
      "\n",
      "\t\t Number of Unique values:  5528\n",
      "\n",
      "\t\t Unique values:  [   1   56   11 ... 5463 2997 3360]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  brand_type\n",
      "\n",
      "\t\t Number of Unique values:  2\n",
      "\n",
      "\t\t Unique values:  ['Established' 'Local']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  category\n",
      "\n",
      "\t\t Number of Unique values:  19\n",
      "\n",
      "\t\t Unique values:  ['Grocery' 'Miscellaneous' 'Bakery' 'Pharmaceutical' 'Packaged Meat'\n",
      " 'Seafood' 'Natural Products' 'Dairy, Juices & Snacks' 'Prepared Food'\n",
      " 'Skin & Hair Care' 'Meat' 'Travel' 'Flowers & Plants' 'Fuel' 'Salads'\n",
      " 'Alcohol' 'Garden' 'Restauarant' 'Vegetables (cut)']\n",
      "   item_id  brand   brand_type       category\n",
      "0        1      1  Established        Grocery\n",
      "1        2      1  Established  Miscellaneous\n",
      "2        3     56        Local         Bakery\n",
      "3        4     56        Local        Grocery\n",
      "4        5     56        Local        Grocery\n"
     ]
    }
   ],
   "source": [
    "# item_data data cleaning and imputation\n",
    "\n",
    "print(\"\\n -------------------------------------- item_data --------------------------------------- \")\n",
    "item_data_columns =  item_data.columns.values\n",
    "print(\"\\n item_data_columns: \", item_data_columns)\n",
    "univariate_analysis(item_data)\n",
    "\n",
    "\n",
    "print(item_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------- customer_demographics_data --------------------------------------- \n",
      "\n",
      " customer_demographics_data_columns:  ['customer_id' 'age_range' 'marital_status' 'rented' 'family_size'\n",
      " 'no_of_children' 'income_bracket']\n",
      "\n",
      " Head: \n",
      "   customer_id age_range marital_status  rented family_size no_of_children  \\\n",
      "0            1       70+        Married       0           2            NaN   \n",
      "1            6     46-55        Married       0           2            NaN   \n",
      "2            7     26-35            NaN       0           3              1   \n",
      "3            8     26-35            NaN       0           4              2   \n",
      "4           10     46-55         Single       0           1            NaN   \n",
      "\n",
      "   income_bracket  \n",
      "0               4  \n",
      "1               5  \n",
      "2               3  \n",
      "3               6  \n",
      "4               5  \n",
      "\n",
      " Number of NaN Entries: \n",
      "customer_id         0\n",
      "age_range           0\n",
      "marital_status    329\n",
      "rented              0\n",
      "family_size         0\n",
      "no_of_children    538\n",
      "income_bracket      0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "customer_id         0\n",
      "age_range           0\n",
      "marital_status    329\n",
      "rented              0\n",
      "family_size         0\n",
      "no_of_children    538\n",
      "income_bracket      0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "        customer_id age_range marital_status      rented family_size  \\\n",
      "count    760.000000       760            431  760.000000         760   \n",
      "unique          NaN         6              2         NaN           5   \n",
      "top             NaN     46-55        Married         NaN           2   \n",
      "freq            NaN       271            317         NaN         303   \n",
      "mean     779.201316       NaN            NaN    0.053947         NaN   \n",
      "std      459.754429       NaN            NaN    0.226063         NaN   \n",
      "min        1.000000       NaN            NaN    0.000000         NaN   \n",
      "25%      382.750000       NaN            NaN    0.000000         NaN   \n",
      "50%      774.500000       NaN            NaN    0.000000         NaN   \n",
      "75%     1187.250000       NaN            NaN    0.000000         NaN   \n",
      "max     1581.000000       NaN            NaN    1.000000         NaN   \n",
      "\n",
      "       no_of_children  income_bracket  \n",
      "count             222      760.000000  \n",
      "unique              3             NaN  \n",
      "top                 1             NaN  \n",
      "freq              107             NaN  \n",
      "mean              NaN        4.715789  \n",
      "std               NaN        2.258817  \n",
      "min               NaN        1.000000  \n",
      "25%               NaN        3.000000  \n",
      "50%               NaN        5.000000  \n",
      "75%               NaN        6.000000  \n",
      "max               NaN       12.000000  \n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  customer_id\n",
      "\n",
      "\t\t Number of Unique values:  760\n",
      "\n",
      "\t\t Unique values:  [   1    6    7    8   10   11   12   13   14   15   17   19   22   27\n",
      "   28   30   31   33   35   36   38   39   40   41   42   45   48   51\n",
      "   52   53   55   58   59   66   67   69   71   72   74   75   78   79\n",
      "   82   83   84   85   87   89   90   92   93   94   97  103  105  107\n",
      "  108  110  112  113  114  119  123  124  128  131  132  134  135  136\n",
      "  138  140  141  142  143  144  149  150  151  153  154  155  158  159\n",
      "  161  162  163  167  168  174  178  179  180  185  186  189  191  192\n",
      "  193  195  197  201  202  204  205  207  209  212  214  223  225  226\n",
      "  227  228  230  231  232  235  238  239  242  243  245  246  248  249\n",
      "  250  251  252  253  259  262  265  266  267  268  269  271  276  277\n",
      "  279  281  283  284  286  287  288  292  293  294  295  297  298  300\n",
      "  303  306  313  314  315  316  317  318  319  320  327  329  333  336\n",
      "  338  339  343  347  348  349  352  353  355  357  361  363  367  368\n",
      "  369  371  374  375  377  378  381  382  383  384  386  389  390  391\n",
      "  392  393  395  397  398  399  400  401  402  414  416  421  422  427\n",
      "  428  429  431  434  436  438  440  441  443  444  446  447  450  455\n",
      "  456  457  460  461  462  463  464  466  467  473  474  475  479  480\n",
      "  481  486  488  490  494  495  497  498  499  501  504  506  507  508\n",
      "  509  510  511  514  515  516  517  520  522  524  525  527  528  530\n",
      "  533  536  537  540  541  544  545  546  549  550  551  552  553  557\n",
      "  558  560  561  564  565  566  569  571  572  574  575  577  579  581\n",
      "  585  586  590  593  595  596  600  602  606  607  608  612  613  615\n",
      "  619  620  622  624  626  627  628  630  634  636  637  641  642  643\n",
      "  648  649  650  653  655  656  657  660  663  665  666  667  668  674\n",
      "  677  679  681  684  686  687  691  693  695  696  698  703  704  708\n",
      "  712  716  719  724  725  726  731  732  735  737  738  740  742  743\n",
      "  744  745  746  748  749  750  751  752  753  754  755  760  762  763\n",
      "  766  774  775  776  779  781  782  784  785  789  792  793  795  796\n",
      "  797  799  800  801  802  806  808  813  817  821  822  823  824  828\n",
      "  829  833  835  837  839  842  844  851  853  855  856  857  859  860\n",
      "  861  864  865  867  871  872  874  876  877  878  879  880  886  888\n",
      "  889  891  892  894  896  897  898  901  902  904  905  909  910  911\n",
      "  912  917  918  921  922  923  925  926  928  929  930  931  932  933\n",
      "  936  937  939  941  944  947  952  955  957  958  959  962  964  965\n",
      "  966  967  970  974  977  978  980  981  982  983  985  988  990  994\n",
      "  997  999 1003 1008 1011 1015 1021 1023 1025 1026 1031 1033 1036 1045\n",
      " 1046 1047 1051 1053 1054 1055 1056 1057 1058 1061 1062 1063 1064 1067\n",
      " 1068 1070 1076 1079 1082 1083 1085 1089 1091 1093 1094 1103 1104 1108\n",
      " 1109 1111 1115 1116 1117 1119 1120 1121 1122 1128 1129 1130 1131 1133\n",
      " 1135 1136 1141 1142 1144 1145 1151 1152 1153 1154 1156 1161 1164 1166\n",
      " 1167 1168 1170 1172 1178 1179 1181 1182 1184 1187 1188 1190 1192 1194\n",
      " 1195 1196 1198 1201 1202 1203 1205 1208 1210 1211 1216 1217 1219 1220\n",
      " 1222 1224 1228 1230 1231 1239 1240 1242 1243 1245 1246 1249 1250 1252\n",
      " 1254 1256 1257 1258 1259 1262 1263 1264 1268 1269 1271 1273 1276 1284\n",
      " 1290 1292 1295 1296 1298 1299 1300 1303 1304 1305 1309 1314 1316 1317\n",
      " 1318 1319 1320 1321 1322 1324 1327 1328 1329 1332 1333 1334 1336 1337\n",
      " 1339 1340 1341 1343 1344 1346 1347 1356 1357 1359 1360 1361 1364 1367\n",
      " 1370 1373 1374 1375 1376 1377 1378 1379 1381 1382 1383 1384 1385 1386\n",
      " 1387 1388 1392 1393 1398 1399 1402 1407 1408 1409 1410 1412 1417 1422\n",
      " 1424 1425 1427 1431 1432 1433 1440 1441 1443 1444 1449 1450 1451 1452\n",
      " 1453 1455 1457 1458 1460 1461 1463 1464 1466 1467 1469 1471 1472 1473\n",
      " 1475 1477 1478 1479 1483 1485 1489 1490 1491 1492 1494 1496 1500 1502\n",
      " 1504 1506 1512 1514 1518 1519 1520 1521 1523 1528 1529 1533 1534 1537\n",
      " 1538 1544 1545 1546 1547 1551 1552 1558 1566 1569 1572 1573 1574 1577\n",
      " 1578 1579 1580 1581]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  age_range\n",
      "\n",
      "\t\t Number of Unique values:  6\n",
      "\n",
      "\t\t Unique values:  ['70+' '46-55' '26-35' '36-45' '18-25' '56-70']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  marital_status\n",
      "\n",
      "\t\t Number of Unique values:  3\n",
      "\n",
      "\t\t Unique values:  ['Married' nan 'Single']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  rented\n",
      "\n",
      "\t\t Number of Unique values:  2\n",
      "\n",
      "\t\t Unique values:  [0 1]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  family_size\n",
      "\n",
      "\t\t Number of Unique values:  5\n",
      "\n",
      "\t\t Unique values:  ['2' '3' '4' '1' '5+']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  no_of_children\n",
      "\n",
      "\t\t Number of Unique values:  4\n",
      "\n",
      "\t\t Unique values:  [nan '1' '2' '3+']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  income_bracket\n",
      "\n",
      "\t\t Number of Unique values:  12\n",
      "\n",
      "\t\t Unique values:  [ 4  5  3  6  1  7  2  8  9 12 10 11]\n"
     ]
    }
   ],
   "source": [
    "# customer_demographics_data data cleaning and imputation\n",
    "current_file = \"\"\n",
    "csv_filename = os.path.join(current_file, '../training_data/customer_demographics.csv')\n",
    "customer_demographics_data = pd.read_csv(csv_filename)\n",
    "\n",
    "print(\"\\n -------------------------------------- customer_demographics_data --------------------------------------- \")\n",
    "customer_demographics_data_columns =  customer_demographics_data.columns.values\n",
    "print(\"\\n customer_demographics_data_columns: \", customer_demographics_data_columns)\n",
    "univariate_analysis(customer_demographics_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_demographics_data.loc[(customer_demographics_data.family_size=='5+'), \"family_size\"] = '5'\n",
    "customer_demographics_data.family_size = pd.to_numeric(customer_demographics_data.family_size)\n",
    "\n",
    "\n",
    "customer_demographics_data.loc[ customer_demographics_data.no_of_children == '3+', \"no_of_children\" ] = 3\n",
    "customer_demographics_data.no_of_children = pd.to_numeric(customer_demographics_data.no_of_children)\n",
    "\n",
    "\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='18-25', \"age_range\"] = 21.5\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='26-35', \"age_range\"] = 30.5\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='36-45', \"age_range\"] = 40.5\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='46-55', \"age_range\"] = 50.5\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='56-70', \"age_range\"] = 63\n",
    "customer_demographics_data.loc[customer_demographics_data.age_range=='70+', \"age_range\"] = 75\n",
    "customer_demographics_data.age_range = pd.to_numeric(customer_demographics_data.age_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Head: \n",
      "   customer_id  age_range marital_status  rented  family_size  no_of_children  \\\n",
      "0            1       75.0        Married       0            2             NaN   \n",
      "1            6       50.5        Married       0            2             NaN   \n",
      "2            7       30.5            NaN       0            3             1.0   \n",
      "3            8       30.5            NaN       0            4             2.0   \n",
      "4           10       50.5         Single       0            1             NaN   \n",
      "\n",
      "   income_bracket  \n",
      "0               4  \n",
      "1               5  \n",
      "2               3  \n",
      "3               6  \n",
      "4               5  \n",
      "\n",
      " Number of NaN Entries: \n",
      "customer_id         0\n",
      "age_range           0\n",
      "marital_status    329\n",
      "rented              0\n",
      "family_size         0\n",
      "no_of_children    538\n",
      "income_bracket      0\n",
      "dtype: int64\n",
      "\n",
      " Number of Null Entries: \n",
      "customer_id         0\n",
      "age_range           0\n",
      "marital_status    329\n",
      "rented              0\n",
      "family_size         0\n",
      "no_of_children    538\n",
      "income_bracket      0\n",
      "dtype: int64\n",
      "\n",
      " Statistical Description: \n",
      "        customer_id   age_range marital_status      rented  family_size  \\\n",
      "count    760.000000  760.000000            431  760.000000   760.000000   \n",
      "unique          NaN         NaN              2         NaN          NaN   \n",
      "top             NaN         NaN        Married         NaN          NaN   \n",
      "freq            NaN         NaN            317         NaN          NaN   \n",
      "mean     779.201316   46.063816            NaN    0.053947     2.161842   \n",
      "std      459.754429   13.756022            NaN    0.226063     1.168929   \n",
      "min        1.000000   21.500000            NaN    0.000000     1.000000   \n",
      "25%      382.750000   40.500000            NaN    0.000000     1.000000   \n",
      "50%      774.500000   50.500000            NaN    0.000000     2.000000   \n",
      "75%     1187.250000   50.500000            NaN    0.000000     3.000000   \n",
      "max     1581.000000   75.000000            NaN    1.000000     5.000000   \n",
      "\n",
      "        no_of_children  income_bracket  \n",
      "count       222.000000      760.000000  \n",
      "unique             NaN             NaN  \n",
      "top                NaN             NaN  \n",
      "freq               NaN             NaN  \n",
      "mean          1.788288        4.715789  \n",
      "std           0.842990        2.258817  \n",
      "min           1.000000        1.000000  \n",
      "25%           1.000000        3.000000  \n",
      "50%           2.000000        5.000000  \n",
      "75%           3.000000        6.000000  \n",
      "max           3.000000       12.000000  \n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  customer_id\n",
      "\n",
      "\t\t Number of Unique values:  760\n",
      "\n",
      "\t\t Unique values:  [   1    6    7    8   10   11   12   13   14   15   17   19   22   27\n",
      "   28   30   31   33   35   36   38   39   40   41   42   45   48   51\n",
      "   52   53   55   58   59   66   67   69   71   72   74   75   78   79\n",
      "   82   83   84   85   87   89   90   92   93   94   97  103  105  107\n",
      "  108  110  112  113  114  119  123  124  128  131  132  134  135  136\n",
      "  138  140  141  142  143  144  149  150  151  153  154  155  158  159\n",
      "  161  162  163  167  168  174  178  179  180  185  186  189  191  192\n",
      "  193  195  197  201  202  204  205  207  209  212  214  223  225  226\n",
      "  227  228  230  231  232  235  238  239  242  243  245  246  248  249\n",
      "  250  251  252  253  259  262  265  266  267  268  269  271  276  277\n",
      "  279  281  283  284  286  287  288  292  293  294  295  297  298  300\n",
      "  303  306  313  314  315  316  317  318  319  320  327  329  333  336\n",
      "  338  339  343  347  348  349  352  353  355  357  361  363  367  368\n",
      "  369  371  374  375  377  378  381  382  383  384  386  389  390  391\n",
      "  392  393  395  397  398  399  400  401  402  414  416  421  422  427\n",
      "  428  429  431  434  436  438  440  441  443  444  446  447  450  455\n",
      "  456  457  460  461  462  463  464  466  467  473  474  475  479  480\n",
      "  481  486  488  490  494  495  497  498  499  501  504  506  507  508\n",
      "  509  510  511  514  515  516  517  520  522  524  525  527  528  530\n",
      "  533  536  537  540  541  544  545  546  549  550  551  552  553  557\n",
      "  558  560  561  564  565  566  569  571  572  574  575  577  579  581\n",
      "  585  586  590  593  595  596  600  602  606  607  608  612  613  615\n",
      "  619  620  622  624  626  627  628  630  634  636  637  641  642  643\n",
      "  648  649  650  653  655  656  657  660  663  665  666  667  668  674\n",
      "  677  679  681  684  686  687  691  693  695  696  698  703  704  708\n",
      "  712  716  719  724  725  726  731  732  735  737  738  740  742  743\n",
      "  744  745  746  748  749  750  751  752  753  754  755  760  762  763\n",
      "  766  774  775  776  779  781  782  784  785  789  792  793  795  796\n",
      "  797  799  800  801  802  806  808  813  817  821  822  823  824  828\n",
      "  829  833  835  837  839  842  844  851  853  855  856  857  859  860\n",
      "  861  864  865  867  871  872  874  876  877  878  879  880  886  888\n",
      "  889  891  892  894  896  897  898  901  902  904  905  909  910  911\n",
      "  912  917  918  921  922  923  925  926  928  929  930  931  932  933\n",
      "  936  937  939  941  944  947  952  955  957  958  959  962  964  965\n",
      "  966  967  970  974  977  978  980  981  982  983  985  988  990  994\n",
      "  997  999 1003 1008 1011 1015 1021 1023 1025 1026 1031 1033 1036 1045\n",
      " 1046 1047 1051 1053 1054 1055 1056 1057 1058 1061 1062 1063 1064 1067\n",
      " 1068 1070 1076 1079 1082 1083 1085 1089 1091 1093 1094 1103 1104 1108\n",
      " 1109 1111 1115 1116 1117 1119 1120 1121 1122 1128 1129 1130 1131 1133\n",
      " 1135 1136 1141 1142 1144 1145 1151 1152 1153 1154 1156 1161 1164 1166\n",
      " 1167 1168 1170 1172 1178 1179 1181 1182 1184 1187 1188 1190 1192 1194\n",
      " 1195 1196 1198 1201 1202 1203 1205 1208 1210 1211 1216 1217 1219 1220\n",
      " 1222 1224 1228 1230 1231 1239 1240 1242 1243 1245 1246 1249 1250 1252\n",
      " 1254 1256 1257 1258 1259 1262 1263 1264 1268 1269 1271 1273 1276 1284\n",
      " 1290 1292 1295 1296 1298 1299 1300 1303 1304 1305 1309 1314 1316 1317\n",
      " 1318 1319 1320 1321 1322 1324 1327 1328 1329 1332 1333 1334 1336 1337\n",
      " 1339 1340 1341 1343 1344 1346 1347 1356 1357 1359 1360 1361 1364 1367\n",
      " 1370 1373 1374 1375 1376 1377 1378 1379 1381 1382 1383 1384 1385 1386\n",
      " 1387 1388 1392 1393 1398 1399 1402 1407 1408 1409 1410 1412 1417 1422\n",
      " 1424 1425 1427 1431 1432 1433 1440 1441 1443 1444 1449 1450 1451 1452\n",
      " 1453 1455 1457 1458 1460 1461 1463 1464 1466 1467 1469 1471 1472 1473\n",
      " 1475 1477 1478 1479 1483 1485 1489 1490 1491 1492 1494 1496 1500 1502\n",
      " 1504 1506 1512 1514 1518 1519 1520 1521 1523 1528 1529 1533 1534 1537\n",
      " 1538 1544 1545 1546 1547 1551 1552 1558 1566 1569 1572 1573 1574 1577\n",
      " 1578 1579 1580 1581]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  age_range\n",
      "\n",
      "\t\t Number of Unique values:  6\n",
      "\n",
      "\t\t Unique values:  [75.  50.5 30.5 40.5 21.5 63. ]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  marital_status\n",
      "\n",
      "\t\t Number of Unique values:  3\n",
      "\n",
      "\t\t Unique values:  ['Married' nan 'Single']\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  rented\n",
      "\n",
      "\t\t Number of Unique values:  2\n",
      "\n",
      "\t\t Unique values:  [0 1]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  family_size\n",
      "\n",
      "\t\t Number of Unique values:  5\n",
      "\n",
      "\t\t Unique values:  [2 3 4 1 5]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  no_of_children\n",
      "\n",
      "\t\t Number of Unique values:  4\n",
      "\n",
      "\t\t Unique values:  [nan  1.  2.  3.]\n",
      "\n",
      "\n",
      " Uniqueness analysis for column:  income_bracket\n",
      "\n",
      "\t\t Number of Unique values:  12\n",
      "\n",
      "\t\t Unique values:  [ 4  5  3  6  1  7  2  8  9 12 10 11]\n"
     ]
    }
   ],
   "source": [
    "univariate_analysis(customer_demographics_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_demographics_data.loc[ (customer_demographics_data.no_of_children.isnull()) \\\n",
    "                                    & (customer_demographics_data.family_size==1), \"no_of_children\"] = 0\n",
    "\n",
    "customer_demographics_data.loc[ (customer_demographics_data.marital_status.isnull()) \\\n",
    "                                    & (customer_demographics_data.family_size==1), \"marital_status\"] = 'Single'\n",
    "\n",
    "customer_demographics_data.loc[ (customer_demographics_data.no_of_children.isnull()) \\\n",
    "                                    & (customer_demographics_data.marital_status=='Married') \\\n",
    "                                    & (customer_demographics_data.family_size==2), \"no_of_children\"] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.9% of customers were with no_of_children > 0 and marital_status = 'Single'.\n",
    "# Keeping the distribution intact during imputation.\n",
    "customer_demographics_data.loc[(customer_demographics_data.marital_status.isnull()) \\\n",
    "                               & (customer_demographics_data.no_of_children > 0), \"marital_status\"] \\\n",
    "= ('Married' if (random.uniform(0, 100) >= 14) else 'Single')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(customer_demographics_data.loc[~(customer_demographics_data.marital_status.isna())\\\n",
    "#                               & (customer_demographics_data.no_of_children.isna())])\n",
    "\n",
    "# It was found on observation that cases where 'no_of_children' was null but 'marital_status' was available all \n",
    "# were 'Single' with family_size = 2.\n",
    "# Assuming that since they were 'Single', they didn't find it necessary to fill the data for 'no_of_children'.\n",
    "# Hence, setting no_of_children for those records as 0.\n",
    "customer_demographics_data.loc[~(customer_demographics_data.marital_status.isna())\\\n",
    "                              & (customer_demographics_data.no_of_children.isna()), \"no_of_children\" ] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id        0\n",
      "age_range          0\n",
      "marital_status    89\n",
      "rented             0\n",
      "family_size        0\n",
      "no_of_children    89\n",
      "income_bracket     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(customer_demographics_data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remaining data, it is observed that 89 customers have missing data remaining and \n",
    "# both 'marital_status' and 'no_of_children' are missing for all of these.\n",
    "# Also, it is observed that all such customers have family_size = 2, and rented = 0.\n",
    "# marital_status \n",
    "\n",
    "# customer_demographics_data.loc[ (customer_demographics_data.marital_status.isna())\\\n",
    "#                               & (customer_demographics_data.no_of_children.isna()) ].family_size.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we find out that people with family_size = 2, for whom data is available, tend to be 'Married' \n",
    "# with following ratios, as per the age_range:\n",
    "# age_range ratio\n",
    "# 21.5     5.0\n",
    "# 30.5    19.0\n",
    "# 40.5    18.5\n",
    "# 50.5    63.0\n",
    "# 63.0     NaN\n",
    "# 75.0     NaN\n",
    "\n",
    "\n",
    "\n",
    "# plt.scatter(customer_demographics_data.family_size, customer_demographics_data.no_of_children,\n",
    "#            alpha=0.01)\n",
    "\n",
    "# cond = ( (customer_demographics_data.marital_status.isna())\\\n",
    "#         & (customer_demographics_data.no_of_children.isna()) )\n",
    "\n",
    "\n",
    "# plt.hist(customer_demographics_data.loc[ cond, \"age_range\" ], density=False)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plt.hist(customer_demographics_data.loc[ ~cond \\\n",
    "#                                         & (customer_demographics_data.family_size == 2)\\\n",
    "#                                         & (customer_demographics_data.marital_status == 'Married'), \"age_range\" ],\n",
    "#          density=False)\n",
    "\n",
    "\n",
    "\n",
    "# plt.hist(customer_demographics_data.loc[ ~cond \\\n",
    "#                                         & (customer_demographics_data.family_size == 2)\\\n",
    "#                                         & (customer_demographics_data.marital_status == 'Single'), \"age_range\" ],\n",
    "#          color = 'r',\n",
    "#          density=False)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# married_with_family_size_2_age_range = customer_demographics_data.loc[ ~cond \\\n",
    "#                                & (customer_demographics_data.family_size == 2) \\\n",
    "#                                & (customer_demographics_data.marital_status == 'Married'), \"age_range\"]\n",
    "\n",
    "# single_with_family_size_2_age_range = customer_demographics_data.loc[ ~cond \\\n",
    "#                                & (customer_demographics_data.family_size == 2) \\\n",
    "#                                & (customer_demographics_data.marital_status == 'Single'), \"age_range\"]\n",
    "\n",
    "# married_with_family_size_2_age_range.value_counts() / single_with_family_size_2_age_range.value_counts()\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the above observation, we fill the remaining values for marital_status = 'Married' and no_of_children = 0 \n",
    "# (given the family_size is 2: the customer and their partner).\n",
    "\n",
    "customer_demographics_data.loc[ (customer_demographics_data.marital_status.isna())\\\n",
    "                              & (customer_demographics_data.no_of_children.isna()) , \n",
    "                               [\"marital_status\", \"no_of_children\"] ] = ['Married', 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_id       0\n",
       "age_range         0\n",
       "marital_status    0\n",
       "rented            0\n",
       "family_size       0\n",
       "no_of_children    0\n",
       "income_bracket    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_demographics_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cleaned_data: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir cleaned_data\n",
    "\n",
    "campaign_data.to_csv(\"cleaned_data/campaign_data.csv\", index=False)\n",
    "coupon_item_mapping_data.to_csv(\"cleaned_data/coupon_item_mapping_data.csv\", index=False)\n",
    "customer_demographics_data.to_csv(\"cleaned_data/customer_demographics_data.csv\", index=False)\n",
    "customer_transaction_data.to_csv(\"cleaned_data/customer_transaction_data.csv\", index=False)\n",
    "item_data.to_csv(\"cleaned_data/item_data.csv\", index=False)\n",
    "train_data.to_csv(\"cleaned_data/train_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: submissions: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir submissions\n",
    "\n",
    "def save_submission(predictions):\n",
    "    submission_df = pd.concat([test_data['id'], pd.Series(predictions, name=\"redemption_status\")], axis=1)\n",
    "\n",
    "    submission_file_name = \"submissions/\" + str(int(time.time())) + \".csv\"\n",
    "\n",
    "    submission_df.to_csv(submission_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='liblinear')\n",
    "classifier.fit(train_data.drop(columns=['id', 'redemption_status']), train_data['redemption_status'])\n",
    "\n",
    "predictions_test = classifier.predict(test_data.drop(columns=['id']))\n",
    "predictions_train = classifier.predict(train_data.drop(columns=['id', 'redemption_status']))\n",
    "predictions_train_probability = classifier.predict_proba(train_data.drop(columns=['id', 'redemption_status']))\n",
    "\n",
    "save_submission(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[77640     0]\n",
      " [  729     0]]\n",
      "0.5746672942333827\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(train_data['redemption_status'], predictions_train))\n",
    "print(roc_auc_score(train_data['redemption_status'], pd.DataFrame(predictions_train_probability)[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.992529</td>\n",
       "      <td>0.007471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.992833</td>\n",
       "      <td>0.007167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.989614</td>\n",
       "      <td>0.010386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.989710</td>\n",
       "      <td>0.010290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.985535</td>\n",
       "      <td>0.014465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78364</td>\n",
       "      <td>0.991113</td>\n",
       "      <td>0.008887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78365</td>\n",
       "      <td>0.993328</td>\n",
       "      <td>0.006672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78366</td>\n",
       "      <td>0.987895</td>\n",
       "      <td>0.012105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78367</td>\n",
       "      <td>0.992751</td>\n",
       "      <td>0.007249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78368</td>\n",
       "      <td>0.989894</td>\n",
       "      <td>0.010106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78369 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.992529  0.007471\n",
       "1      0.992833  0.007167\n",
       "2      0.989614  0.010386\n",
       "3      0.989710  0.010290\n",
       "4      0.985535  0.014465\n",
       "...         ...       ...\n",
       "78364  0.991113  0.008887\n",
       "78365  0.993328  0.006672\n",
       "78366  0.987895  0.012105\n",
       "78367  0.992751  0.007249\n",
       "78368  0.989894  0.010106\n",
       "\n",
       "[78369 rows x 2 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(predictions_train_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
